{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Numpy Parsing\n",
    "\n",
    "Joeri R. Hermans                    \n",
    "*Departement of Data Science & Knowledge Engineering*          \n",
    "*Maastricht University, The Netherlands*           \n",
    "\n",
    "This notebook will show you how to parse a collection of Numpy files straight from HDFS into a Spark Dataframe.\n",
    "\n",
    "## Cluster Configuration\n",
    "\n",
    "In the following sections, we set up the cluster properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.sql import Row\n",
    "\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "\n",
    "# Use the DataBricks AVRO reader.\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-avro_2.11:3.2.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of desired executors: 20\n",
      "Number of desired processes / executor: 1\n",
      "Total number of workers: 20\n"
     ]
    }
   ],
   "source": [
    "# Modify these variables according to your needs.\n",
    "application_name = \"Distributed Numpy Parsing\"\n",
    "using_spark_2 = False\n",
    "local = False\n",
    "\n",
    "if local:\n",
    "    # Tell master to use local resources.\n",
    "    master = \"local[*]\"\n",
    "    num_processes = 3\n",
    "    num_executors = 1\n",
    "else:\n",
    "    # Tell master to use YARN.\n",
    "    master = \"yarn-client\"\n",
    "    num_executors = 20\n",
    "    num_processes = 1\n",
    "\n",
    "# This variable is derived from the number of cores and executors,\n",
    "# and will be used to assign the number of model trainers.\n",
    "num_workers = num_executors * num_processes\n",
    "\n",
    "print(\"Number of desired executors: \" + `num_executors`)\n",
    "print(\"Number of desired processes / executor: \" + `num_processes`)\n",
    "print(\"Total number of workers: \" + `num_workers`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Do not change anything here.\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.app.name\", application_name)\n",
    "conf.set(\"spark.master\", master)\n",
    "conf.set(\"spark.executor.cores\", `num_processes`)\n",
    "conf.set(\"spark.executor.instances\", `num_executors`)\n",
    "conf.set(\"spark.executor.memory\", \"5g\")\n",
    "conf.set(\"spark.locality.wait\", \"0\")\n",
    "conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "conf.set(\"spark.kryoserializer.buffer.max\", \"2000\")\n",
    "conf.set(\"spark.executor.heartbeatInterval\", \"6000s\")\n",
    "conf.set(\"spark.network.timeout\", \"10000000s\")\n",
    "conf.set(\"spark.shuffle.spill\", \"true\")\n",
    "conf.set(\"spark.driver.memory\", \"10g\")\n",
    "conf.set(\"spark.driver.maxResultSize\", \"10g\")\n",
    "\n",
    "# Check if the user is running Spark 2.0 +\n",
    "if using_spark_2:\n",
    "    sc = SparkSession.builder.config(conf=conf) \\\n",
    "                     .appName(application_name) \\\n",
    "                     .getOrCreate()\n",
    "else:\n",
    "    # Create the Spark context.\n",
    "    sc = SparkContext(conf=conf)\n",
    "    # Add the missing imports\n",
    "    from pyspark import SQLContext\n",
    "    sqlContext = SQLContext(sc)\n",
    "\n",
    "# Check if we are using Spark 2.0\n",
    "if using_spark_2:\n",
    "    reader = sc\n",
    "else:\n",
    "    reader = sqlContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining the required file-paths\n",
    "\n",
    "Basically what we are going to do now, is obtain a lists of file paths (*.npy) which we will map with a custom lambda function to read all the data into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the command that needs to be executed, this will list all the numpy files in the specified directory.\n",
    "cmd = \"hdfs dfs -ls /user/jhermans/data/cms/RelValWjet_Pt_3000_3500_13_GEN-SIM-RECO_evt3150/*.npy | awk '{print $NF}'\"\n",
    "# Fetch the output of the command, and construct a list.\n",
    "output = os.popen(cmd).read()\n",
    "file_paths = output.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Spark Dataframe from the specified list\n",
    "\n",
    "Before we convert to a list to a Spark Dataframe, we first need to specify the schema. We do this by converting every element in the list to a Spark row. Afterwards, Spark will be able to automatically infer the schema of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "for path in file_paths:\n",
    "    row = Row(**{'path': path})\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are able to create the Spark DataFrame. Note, for Spark 2.0 use `spark.` instead of `sqlContext.`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of paths to be parsed: 393\n",
      "root\n",
      " |-- path: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = sqlContext.createDataFrame(rows)\n",
    "# Repartition the dataset for increased parallelism.\n",
    "df = df.repartition(20)\n",
    "\n",
    "print(\"Number of paths to be parsed: \" + str(df.count()))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(path=u'/user/jhermans/data/cms/RelValWjet_Pt_3000_3500_13_GEN-SIM-RECO_evt3150/trackparams220.npy')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example content of the dataframe.\n",
    "df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing your Numpy files\n",
    "\n",
    "This is a fairly straightforward operation where we basically map all the file paths using a custom lambda function to read the numpy files from HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns: 190\n",
      "First five columns: \n",
      "sis_25_x\n",
      "normalizedChi2\n",
      "sis_25_z\n",
      "sis_25_y\n",
      "sis_48_x\n"
     ]
    }
   ],
   "source": [
    "# Development cell, this will be executed in the lambdas.\n",
    "\n",
    "import pydoop.hdfs as hdfs\n",
    "\n",
    "with hdfs.open(file_paths[0]) as f:\n",
    "    data = np.load(f)\n",
    "\n",
    "# Obtain the fields (columns) of your numpy data.\n",
    "fields = []\n",
    "for k in data[0].dtype.fields:\n",
    "    fields.append(k)\n",
    "    \n",
    "print(\"Number of columns: \" + str(len(data.dtype.fields)))\n",
    "\n",
    "print(\"First five columns: \")\n",
    "i = 0\n",
    "for k in data.dtype.fields:\n",
    "    print(k)\n",
    "    i += 1\n",
    "    if i == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a working prototype, let's construct a Spark mapper which will fetch the data in a distributed manner from HDFS. Note that if you would like to adjust the data in any way after reading, you can do so by modifying the lambda function, or executing another map after the data has been read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- TrackId: long (nullable = true)\n",
      " |-- charge: long (nullable = true)\n",
      " |-- chi2: double (nullable = true)\n",
      " |-- d0: double (nullable = true)\n",
      " |-- dsz: double (nullable = true)\n",
      " |-- dxy: double (nullable = true)\n",
      " |-- dz: double (nullable = true)\n",
      " |-- eta: double (nullable = true)\n",
      " |-- evt: long (nullable = true)\n",
      " |-- lambda: double (nullable = true)\n",
      " |-- lumi: long (nullable = true)\n",
      " |-- ndof: double (nullable = true)\n",
      " |-- normalizedChi2: double (nullable = true)\n",
      " |-- p: double (nullable = true)\n",
      " |-- phi: double (nullable = true)\n",
      " |-- pix_0_x: double (nullable = true)\n",
      " |-- pix_0_y: double (nullable = true)\n",
      " |-- pix_0_z: double (nullable = true)\n",
      " |-- pix_1_x: double (nullable = true)\n",
      " |-- pix_1_y: double (nullable = true)\n",
      " |-- pix_1_z: double (nullable = true)\n",
      " |-- pix_2_x: double (nullable = true)\n",
      " |-- pix_2_y: double (nullable = true)\n",
      " |-- pix_2_z: double (nullable = true)\n",
      " |-- pix_3_x: double (nullable = true)\n",
      " |-- pix_3_y: double (nullable = true)\n",
      " |-- pix_3_z: double (nullable = true)\n",
      " |-- pix_4_x: double (nullable = true)\n",
      " |-- pix_4_y: double (nullable = true)\n",
      " |-- pix_4_z: double (nullable = true)\n",
      " |-- pt: double (nullable = true)\n",
      " |-- px: double (nullable = true)\n",
      " |-- py: double (nullable = true)\n",
      " |-- pz: double (nullable = true)\n",
      " |-- qoverp: double (nullable = true)\n",
      " |-- run: long (nullable = true)\n",
      " |-- sis_0_x: double (nullable = true)\n",
      " |-- sis_0_y: double (nullable = true)\n",
      " |-- sis_0_z: double (nullable = true)\n",
      " |-- sis_10_x: double (nullable = true)\n",
      " |-- sis_10_y: double (nullable = true)\n",
      " |-- sis_10_z: double (nullable = true)\n",
      " |-- sis_11_x: double (nullable = true)\n",
      " |-- sis_11_y: double (nullable = true)\n",
      " |-- sis_11_z: double (nullable = true)\n",
      " |-- sis_12_x: double (nullable = true)\n",
      " |-- sis_12_y: double (nullable = true)\n",
      " |-- sis_12_z: double (nullable = true)\n",
      " |-- sis_13_x: double (nullable = true)\n",
      " |-- sis_13_y: double (nullable = true)\n",
      " |-- sis_13_z: double (nullable = true)\n",
      " |-- sis_14_x: double (nullable = true)\n",
      " |-- sis_14_y: double (nullable = true)\n",
      " |-- sis_14_z: double (nullable = true)\n",
      " |-- sis_15_x: double (nullable = true)\n",
      " |-- sis_15_y: double (nullable = true)\n",
      " |-- sis_15_z: double (nullable = true)\n",
      " |-- sis_16_x: double (nullable = true)\n",
      " |-- sis_16_y: double (nullable = true)\n",
      " |-- sis_16_z: double (nullable = true)\n",
      " |-- sis_17_x: double (nullable = true)\n",
      " |-- sis_17_y: double (nullable = true)\n",
      " |-- sis_17_z: double (nullable = true)\n",
      " |-- sis_18_x: double (nullable = true)\n",
      " |-- sis_18_y: double (nullable = true)\n",
      " |-- sis_18_z: double (nullable = true)\n",
      " |-- sis_19_x: double (nullable = true)\n",
      " |-- sis_19_y: double (nullable = true)\n",
      " |-- sis_19_z: double (nullable = true)\n",
      " |-- sis_1_x: double (nullable = true)\n",
      " |-- sis_1_y: double (nullable = true)\n",
      " |-- sis_1_z: double (nullable = true)\n",
      " |-- sis_20_x: double (nullable = true)\n",
      " |-- sis_20_y: double (nullable = true)\n",
      " |-- sis_20_z: double (nullable = true)\n",
      " |-- sis_21_x: double (nullable = true)\n",
      " |-- sis_21_y: double (nullable = true)\n",
      " |-- sis_21_z: double (nullable = true)\n",
      " |-- sis_22_x: double (nullable = true)\n",
      " |-- sis_22_y: double (nullable = true)\n",
      " |-- sis_22_z: double (nullable = true)\n",
      " |-- sis_23_x: double (nullable = true)\n",
      " |-- sis_23_y: double (nullable = true)\n",
      " |-- sis_23_z: double (nullable = true)\n",
      " |-- sis_24_x: double (nullable = true)\n",
      " |-- sis_24_y: double (nullable = true)\n",
      " |-- sis_24_z: double (nullable = true)\n",
      " |-- sis_25_x: double (nullable = true)\n",
      " |-- sis_25_y: double (nullable = true)\n",
      " |-- sis_25_z: double (nullable = true)\n",
      " |-- sis_26_x: double (nullable = true)\n",
      " |-- sis_26_y: double (nullable = true)\n",
      " |-- sis_26_z: double (nullable = true)\n",
      " |-- sis_27_x: double (nullable = true)\n",
      " |-- sis_27_y: double (nullable = true)\n",
      " |-- sis_27_z: double (nullable = true)\n",
      " |-- sis_28_x: double (nullable = true)\n",
      " |-- sis_28_y: double (nullable = true)\n",
      " |-- sis_28_z: double (nullable = true)\n",
      " |-- sis_29_x: double (nullable = true)\n",
      " |-- sis_29_y: double (nullable = true)\n",
      " |-- sis_29_z: double (nullable = true)\n",
      " |-- sis_2_x: double (nullable = true)\n",
      " |-- sis_2_y: double (nullable = true)\n",
      " |-- sis_2_z: double (nullable = true)\n",
      " |-- sis_30_x: double (nullable = true)\n",
      " |-- sis_30_y: double (nullable = true)\n",
      " |-- sis_30_z: double (nullable = true)\n",
      " |-- sis_31_x: double (nullable = true)\n",
      " |-- sis_31_y: double (nullable = true)\n",
      " |-- sis_31_z: double (nullable = true)\n",
      " |-- sis_32_x: double (nullable = true)\n",
      " |-- sis_32_y: double (nullable = true)\n",
      " |-- sis_32_z: double (nullable = true)\n",
      " |-- sis_33_x: double (nullable = true)\n",
      " |-- sis_33_y: double (nullable = true)\n",
      " |-- sis_33_z: double (nullable = true)\n",
      " |-- sis_34_x: double (nullable = true)\n",
      " |-- sis_34_y: double (nullable = true)\n",
      " |-- sis_34_z: double (nullable = true)\n",
      " |-- sis_35_x: double (nullable = true)\n",
      " |-- sis_35_y: double (nullable = true)\n",
      " |-- sis_35_z: double (nullable = true)\n",
      " |-- sis_36_x: double (nullable = true)\n",
      " |-- sis_36_y: double (nullable = true)\n",
      " |-- sis_36_z: double (nullable = true)\n",
      " |-- sis_37_x: double (nullable = true)\n",
      " |-- sis_37_y: double (nullable = true)\n",
      " |-- sis_37_z: double (nullable = true)\n",
      " |-- sis_38_x: double (nullable = true)\n",
      " |-- sis_38_y: double (nullable = true)\n",
      " |-- sis_38_z: double (nullable = true)\n",
      " |-- sis_39_x: double (nullable = true)\n",
      " |-- sis_39_y: double (nullable = true)\n",
      " |-- sis_39_z: double (nullable = true)\n",
      " |-- sis_3_x: double (nullable = true)\n",
      " |-- sis_3_y: double (nullable = true)\n",
      " |-- sis_3_z: double (nullable = true)\n",
      " |-- sis_40_x: double (nullable = true)\n",
      " |-- sis_40_y: double (nullable = true)\n",
      " |-- sis_40_z: double (nullable = true)\n",
      " |-- sis_41_x: double (nullable = true)\n",
      " |-- sis_41_y: double (nullable = true)\n",
      " |-- sis_41_z: double (nullable = true)\n",
      " |-- sis_42_x: double (nullable = true)\n",
      " |-- sis_42_y: double (nullable = true)\n",
      " |-- sis_42_z: double (nullable = true)\n",
      " |-- sis_43_x: double (nullable = true)\n",
      " |-- sis_43_y: double (nullable = true)\n",
      " |-- sis_43_z: double (nullable = true)\n",
      " |-- sis_44_x: double (nullable = true)\n",
      " |-- sis_44_y: double (nullable = true)\n",
      " |-- sis_44_z: double (nullable = true)\n",
      " |-- sis_45_x: double (nullable = true)\n",
      " |-- sis_45_y: double (nullable = true)\n",
      " |-- sis_45_z: double (nullable = true)\n",
      " |-- sis_46_x: double (nullable = true)\n",
      " |-- sis_46_y: double (nullable = true)\n",
      " |-- sis_46_z: double (nullable = true)\n",
      " |-- sis_47_x: double (nullable = true)\n",
      " |-- sis_47_y: double (nullable = true)\n",
      " |-- sis_47_z: double (nullable = true)\n",
      " |-- sis_48_x: double (nullable = true)\n",
      " |-- sis_48_y: double (nullable = true)\n",
      " |-- sis_48_z: double (nullable = true)\n",
      " |-- sis_49_x: double (nullable = true)\n",
      " |-- sis_49_y: double (nullable = true)\n",
      " |-- sis_49_z: double (nullable = true)\n",
      " |-- sis_4_x: double (nullable = true)\n",
      " |-- sis_4_y: double (nullable = true)\n",
      " |-- sis_4_z: double (nullable = true)\n",
      " |-- sis_5_x: double (nullable = true)\n",
      " |-- sis_5_y: double (nullable = true)\n",
      " |-- sis_5_z: double (nullable = true)\n",
      " |-- sis_6_x: double (nullable = true)\n",
      " |-- sis_6_y: double (nullable = true)\n",
      " |-- sis_6_z: double (nullable = true)\n",
      " |-- sis_7_x: double (nullable = true)\n",
      " |-- sis_7_y: double (nullable = true)\n",
      " |-- sis_7_z: double (nullable = true)\n",
      " |-- sis_8_x: double (nullable = true)\n",
      " |-- sis_8_y: double (nullable = true)\n",
      " |-- sis_8_z: double (nullable = true)\n",
      " |-- sis_9_x: double (nullable = true)\n",
      " |-- sis_9_y: double (nullable = true)\n",
      " |-- sis_9_z: double (nullable = true)\n",
      " |-- theta: double (nullable = true)\n",
      " |-- vx: double (nullable = true)\n",
      " |-- vy: double (nullable = true)\n",
      " |-- vz: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def parse(iterator):\n",
    "    rows = []\n",
    "    \n",
    "    # MODIFY TO YOUR NEEDS IF NECESSARY\n",
    "    for row in iterator:\n",
    "        path = row['path']\n",
    "        # Load the file from HFDS.\n",
    "        with hdfs.open(path) as f:\n",
    "            data = np.load(f)\n",
    "        # Add all rows in current path.\n",
    "        for r in data:\n",
    "            d = {}\n",
    "            for f in fields:\n",
    "                d[f] = r[f].item()\n",
    "            rows.append(Row(**d))\n",
    "        \n",
    "    return iter(rows)\n",
    "\n",
    "# Apply the lambda function.\n",
    "dataset = df.rdd.mapPartitions(parse).toDF()\n",
    "dataset.printSchema()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
