{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kafka and Spark High Throughput Deep Learning Production Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Joeri Hermans** (Technical Student, IT-DB-SAS, CERN)             \n",
    "*Departement of Knowledge Engineering*         \n",
    "*Maastricht University, The Netherlands*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 November 2016\r\n"
     ]
    }
   ],
   "source": [
    "!(date +%d\\ %B\\ %G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will inform the reader how to set up a production ready machine learning pipeline using [Apache Kafka](https://kafka.apache.org) and [Apache Spark](https://spark.apache.org), together with our Distributed Deep Learning framework [Distributed Keras](https://github.com/JoeriHermans/dist-keras) which is built using [Keras](https://keras.io).\n",
    "\n",
    "***Note before starting this notebook: *** Do not forget to run the Kafka producer (as explained in this notebook)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "- [Introduction and problem statement](#Problem-statement)\n",
    "- [Preliminaries](#Preliminaries)\n",
    "   - [Installation and requirements](#Installation-and-requirements)\n",
    "   - [Pretrained model](#Pretrained-model)\n",
    "   - [Kafka producer](#Kafka-producer)\n",
    "- [Usage](#Distributed-Keras:-a-practicle-example)\n",
    "- [Experiments](#Experiments)\n",
    "- [Conclusion](#Conclusion)\n",
    "- [Acknowledgments](#Acknowledgments)\n",
    "- [References](#References)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import Normalizer\n",
    "\n",
    "from distkeras.trainers import *\n",
    "from distkeras.predictors import *\n",
    "from distkeras.transformers import *\n",
    "from distkeras.evaluators import *\n",
    "from distkeras.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem statement\n",
    "\n",
    "The problem of building an efficient machine learning production pipeline is quite similar to building an efficient training procedure. However, in contrast to the training procedure, in production (model serving) most of this data will arrive in a streaming fashion. Usually, one just reads from a particular source using Spark Streaming. However, intergration with Apache Kafka is also possible. Kafka allows us to scale our streaming application if a bottleneck would occur. At CERN we employ Apache Kafka with different use-cases in [IT](https://db-blog.web.cern.ch/blog/prasanth-kothuri/2016-10-benchmarking-apache-kafka-openstack-vms) (IT Group), [BE](https://indico.cern.ch/event/533714/contributions/2173938/attachments/1292041/1924841/CALS2-Hadoop-IT.pdf) (Beams Group), and ATLAS.\n",
    "\n",
    "However, building a distributed streaming application has some practical considerations as mentioned in [1]. This includes specifying the *retention* (i.e., how much time is the data allowed to stay in the buffer, or what is the maximum size of the buffer before discarding older data) of the data in your buffer, usage of *compression*, number of *brokers*, *partitions*, and how to throttle incoming data. Of course, these settings are always application and infrastructure depended. But since this is a general-purpose framework, we will show in the following sections how to build a scalable deep learning production (model serving) pipeline using the technologies mentioned above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "\n",
    "### Installation and requirements\n",
    "\n",
    "#### Cluster requirements\n",
    "\n",
    "We will assume that you will already have a running Kafka and Spark cluster. Furthermore, in order to run this example, we require that the topic **\"Machine_Learning\"** is available on this Kafka cluster.\n",
    "\n",
    "#### Kafka Python\n",
    "\n",
    "In order to manage your Python dependencies, it is recommended to install a Python distribution like [Anaconda](https://www.continuum.io/downloads). In the following sections, we assume that Spark is already added to your PATH variable. In order to run our Kafka producer (located in the *examples* directory). We first need [Kafka Python](https://github.com/dpkp/kafka-python). This is done by simply running Pip in your shell:\n",
    "\n",
    "```pip install kafka-python```\n",
    "\n",
    "### Pretrained model\n",
    "\n",
    "In order to run a production classification pipeline you should have access to a trained model. Keras provides an API to load and store trained models. The same procedures can be used with Distributed Keras and Spark to load a pretrained model for production use-cases. However, in this example, we will construct a Neural Network with randomly initialized weights (which will simulate such a pretrained model). The structure of the model (input and output data) will be equivalent to the neural network in the *workflow notebook*. So if anyone wants to use the distributed training methods described in the workflow notebook to train a model, and afterwards save it to use the trained model in this notebook, you should not experience any problems. Just make sure the model variable is set to your trained Keras model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As defined in the *workflow* notebook, our neural network will use 30 features and will be trained to classify two classes (signal and background)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_features = 30\n",
    "nb_classes = 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As described above, we construct a randomly initialized neural network to simulate a pretrained network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_1 (Dense)                  (None, 500)           15500       dense_input_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 500)           0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 500)           0           activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 1000)          501000      dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 1000)          0           dense_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 1000)          0           activation_2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 500)           500500      dropout_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, 500)           0           dense_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 2)             1002        activation_3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_4 (Activation)        (None, 2)             0           dense_4[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 1018002\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(500, input_shape=(nb_features,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(1000))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(500))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kafka producer\n",
    "\n",
    "In order to run the Kafka producer, change the directory to the examples directory. Next, fetch the address of a bootstrap server. Once you have this address, run the following command in a seperate shell to run the Kafka producer:\n",
    "\n",
    "```python kafka_producer.py [bootstrap_server]```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, please modify the required parameters according to your requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Modify these variables according to your needs.\n",
    "application_name = \"Distributed Keras Kafka Pipeline\"\n",
    "using_spark_2 = False\n",
    "local = False\n",
    "if local:\n",
    "    # Tell master to use local resources.\n",
    "    master = \"local[*]\"\n",
    "    num_cores = 3\n",
    "    num_executors = 1\n",
    "else:\n",
    "    # Tell master to use YARN.\n",
    "    master = \"yarn-client\"\n",
    "    num_executors = 8\n",
    "    num_cores = 2\n",
    "# Define Kafka specific metrics.\n",
    "zk = \"zookeeper_host:2181\";             # ZooKeeper address\n",
    "topic = \"Machine_Learning\"              # Topic name\n",
    "consumer_name = \"dist-keras-consumer\"   # Consumer identifier\n",
    "# Define Spark streaming specific parameters.\n",
    "batch_duriation = 10 # In seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will allocate a Spark Context (sc) with a Spark Streaming Context (ssc) using the parameters you provided above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conf = SparkConf()\n",
    "conf.set(\"spark.app.name\", application_name)\n",
    "conf.set(\"spark.master\", master)\n",
    "conf.set(\"spark.executor.cores\", `num_cores`)\n",
    "conf.set(\"spark.executor.instances\", `num_executors`)\n",
    "conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\");\n",
    "\n",
    "# Check if the user is running Spark 2.0 +\n",
    "if using_spark_2:\n",
    "    sc = SparkSession.builder.config(conf=conf) \\\n",
    "            .appName(application_name) \\\n",
    "            .getOrCreate()\n",
    "else:\n",
    "    # Create the Spark context.\n",
    "    sc = SparkContext(conf=conf)\n",
    "    # Add the missing imports\n",
    "    from pyspark import SQLContext\n",
    "    sqlContext = SQLContext(sc)\n",
    "# Allocate the streaming context with a batch duration of 10 seconds.\n",
    "ssc = StreamingContext(sc, batch_duriation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we allocate a Kafka Stream using the previously defined parameters. However, the final parameter, which is passed as a dictionary, will tell the consumer group to read from (in this case) 3 different partitions at once.\n",
    "\n",
    "For additional and more detailed information on Spark's Kafka API, we will refer to their documentation [http://spark.apache.org/docs/latest/streaming-kafka-0-8-integration.html](http://spark.apache.org/docs/latest/streaming-kafka-0-8-integration.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Allocate a Kafka stream.\n",
    "kafkaStream = KafkaUtils.createStream(ssc, zk, consumer_name, {topic: 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(df):\n",
    "    \"\"\"This method will add a prediction column to the specified DataFrame using the pretrained model.\"\"\"\n",
    "    predictor = ModelPredictor(keras_model=model, features_col=\"features_normalized\", output_col=\"prediction\")\n",
    "    \n",
    "    return predictor.predict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def post_process(df):\n",
    "    \"\"\"\n",
    "    Will add a column to the specified DataFrame by converting the raw\n",
    "    model prediction (which is an array) to a predicted class (identifier by an index).\n",
    "    Since we only have two classes, the output dimension is 2. This will cause the\n",
    "    LabelIndexTransformer to output a 0 or a 1 given the raw neural network classification.\n",
    "    \"\"\"\n",
    "    transformer = LabelIndexTransformer(output_dim=2, input_col=\"prediction\", output_col=\"predicted_index\")\n",
    "    \n",
    "    return transformer.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_dataframe(df):\n",
    "    \"\"\"\n",
    "    Takes the specified dataframe and add two columns:\n",
    "    \n",
    "    1. features\n",
    "       Every row will hold a vector of the specified features.\n",
    "    2. features_normalized\n",
    "       Every row will hold a normalized vector of features based\n",
    "       on the features vector created before.\n",
    "    \"\"\"\n",
    "    features = df.columns\n",
    "    features.remove('EventId')\n",
    "    vector_assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
    "    df = vector_assembler.transform(df)\n",
    "    normalizer = Normalizer(inputCol=\"features\", outputCol=\"features_normalized\", p=2.0)\n",
    "    df = normalizer.transform(df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In *process_instances* we will process the incoming RDD's into predictions. Of course, since there is no real goal to this notebook besides demonstration purposes, we just print the number of instances which were classified as \"signal\" by the pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_instances(rdd):\n",
    "    # Check if there is new data available.\n",
    "    if not rdd.isEmpty():\n",
    "        df = rdd.toDF()             # Convert the RDD to a Spark DataFrame.\n",
    "        df = prepare_dataframe(df)  # Create a feature column and normalize the batch.\n",
    "        df = predict(df)            # Add the raw Neural Network predictions.\n",
    "        df = post_process(df)       # Convert the raw Neural Network predictions to a class (index).\n",
    "        # Extract the instances which are interesting (signal).\n",
    "        df = df.filter(df['predicted_index'] == 0)\n",
    "        # TODO: Do something with your DataFrame (e.g., storing to HDFS).\n",
    "        print(df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fetch the raw instances from the Kafka stream.\n",
    "raw_instances = kafkaStream.map(lambda x: x[1])\n",
    "# Convert the raw instances (which are JSON strings) to Spark rows.\n",
    "instances = raw_instances.map(json_to_dataframe_row)\n",
    "# Process every RDD in the DStream.\n",
    "instances.foreachRDD(process_instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33023\n",
      "46801\n",
      "45446\n",
      "48116\n",
      "22459\n",
      "45999\n"
     ]
    }
   ],
   "source": [
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "\n",
    "TODO\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In this notebook we demonstrated how to construct a high throughput model serving pipeline using Apache Spark, Apache Kafka and Distributed Keras. Furthermore, we also showed that this infrastructure provides an easily scalable approach for production use-cases. However, since Distributed Keras is still being developed, some bugs might still show up. So please notify us when any of these occur on your system.\n",
    "\n",
    "**Contact**: [joeri.hermans@cern.ch](mailto:joeri.hermans@cern.ch)\n",
    "             [luca.canali@cern.ch](mailto:luca.canali@cern.ch)\n",
    "             [zbigniew.baranowski@cern.ch](mailto:zbigniew.baranowski@cern.ch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgements\n",
    "\n",
    "Many thanks to Zbigniew Baranowski and Luca Canali of the IT-DB group for their collaboration on this work."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
