{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR-10 Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Joeri Hermans** (Technical Student, IT-DB-SAS, CERN)             \n",
    "*Departement of Data Science & Knowledge Engineering*         \n",
    "*Maastricht University, The Netherlands*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we download the CIFAR-10 dataset, and prepare it in such a way it can be processed by Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import cPickle as pickle\n",
    "\n",
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "from distkeras.trainers import *\n",
    "from distkeras.predictors import *\n",
    "from distkeras.transformers import *\n",
    "from distkeras.evaluators import *\n",
    "from distkeras.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading and decompressing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2017-01-26 15:42:04--  https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "Resolving www.cs.toronto.edu... 128.100.3.30\n",
      "Connecting to www.cs.toronto.edu|128.100.3.30|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 170498071 (163M) [application/x-gzip]\n",
      "Saving to: “cifar-10-python.tar.gz”\n",
      "\n",
      "100%[======================================>] 170,498,071 4.88M/s   in 33s     \n",
      "\n",
      "2017-01-26 15:42:40 (4.89 MB/s) - “cifar-10-python.tar.gz” saved [170498071/170498071]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!rm cifar-10-python.tar.gz\n",
    "!rm -r cifar-10-batches-py\n",
    "!wget https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cifar-10-batches-py/\n",
      "cifar-10-batches-py/data_batch_4\n",
      "cifar-10-batches-py/readme.html\n",
      "cifar-10-batches-py/test_batch\n",
      "cifar-10-batches-py/data_batch_3\n",
      "cifar-10-batches-py/batches.meta\n",
      "cifar-10-batches-py/data_batch_2\n",
      "cifar-10-batches-py/data_batch_5\n",
      "cifar-10-batches-py/data_batch_1\n"
     ]
    }
   ],
   "source": [
    "!tar -xvzf cifar-10-python.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset in memory for further processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training instances: 50000\n"
     ]
    }
   ],
   "source": [
    "# Define the required datastructures.\n",
    "training_instances = []\n",
    "training_labels = []\n",
    "\n",
    "# Iterate through all training batches, and load them in memory.\n",
    "for i in range(1, 6):\n",
    "    path = \"cifar-10-batches-py/data_batch_\" + str(i)\n",
    "    fd = open(path, \"rb\")\n",
    "    d = pickle.load(fd)\n",
    "    fd.close()\n",
    "    # Add the training data to our datastructures.\n",
    "    num_instances = len(d['data'])\n",
    "    for j in range(0, num_instances):\n",
    "        training_instances.append(d['data'][j])\n",
    "        training_labels.append(d['labels'][j])\n",
    "        \n",
    "print(\"Number of training instances: \" + str(len(training_instances)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test instances: 10000\n"
     ]
    }
   ],
   "source": [
    "# Define the reuiqred datastructures.\n",
    "test_instances = []\n",
    "test_labels = []\n",
    "\n",
    "# Load the test batch.\n",
    "path = \"cifar-10-batches-py/test_batch\"\n",
    "fd = open(path, \"rb\")\n",
    "d = pickle.load(fd)\n",
    "fd.close()\n",
    "# Add the testset to our datastructures.\n",
    "num_instances = len(d['data'])\n",
    "for j in range(0, num_instances):\n",
    "    test_instances.append(d['data'][j])\n",
    "    test_labels.append(d['labels'][j])\n",
    "    \n",
    "print(\"Number of test instances: \" + str(len(test_instances)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we have the training and test set in memory. At this point we basically have 2 options to prepare it for Apache Spark. First, we simply \"parallelize\" the data, and continue from there. However, this requires some additional logic. The second approach is to write it to a file which Spark will be able to read (CSV, Parquet, Avro...). Due to the simplicity of the second approach, we will choose to write the contents of our datastructures in a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns: 3073\n"
     ]
    }
   ],
   "source": [
    "# First, prepare the column names.\n",
    "columns = ['label']\n",
    "# Now, add the pixel column names. Note, first 1024 pixels are red, then green and finally blue.\n",
    "for c in ['r','g','b']:\n",
    "    for i in range(0, 1024):\n",
    "        column_name = \"p_\" + str(i) + \"_\" + c\n",
    "        columns.append(column_name)\n",
    "        \n",
    "# Now, we should have 3072 (data) + 1 (label) column names.\n",
    "print(\"Number of columns: \" + str(len(columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size training set: 50000\n",
      "Size test set: 10000\n"
     ]
    }
   ],
   "source": [
    "training_set = []\n",
    "test_set = []\n",
    "\n",
    "# Prepare the training set.\n",
    "for i in range(0, len(training_instances)):\n",
    "    row = np.insert(training_instances[i], 0, training_labels[i])\n",
    "    training_set.append(row)\n",
    "\n",
    "# Prepare the test set.\n",
    "for i in range(0, len(test_instances)):\n",
    "    row = np.insert(test_instances[i], 0, test_labels[i])\n",
    "    test_set.append(row)\n",
    "    \n",
    "print(\"Size training set: \" + str(len(training_set)))\n",
    "print(\"Size test set: \" + str(len(test_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save(path, columns, dataset):\n",
    "    with open(path, 'wb') as f:\n",
    "        w = csv.writer(f)\n",
    "        # Write the columns.\n",
    "        w.writerow(columns)\n",
    "        # Iterate through all instances in the training set.\n",
    "        n = len(dataset)\n",
    "        for i in range(0, n):\n",
    "            w.writerow(dataset[i].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save the datasets to disk.\n",
    "save(\"cifar-10-training.csv\", columns, training_set)\n",
    "save(\"cifar-10-test.csv\", columns, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cifar-10-test.csv\r\n",
      "cifar-10-training.csv\r\n"
     ]
    }
   ],
   "source": [
    "# Confirming that produced CSV's are present\n",
    "!ls | grep cifar | grep csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted data/cifar-10-training.csv\n",
      "Deleted data/cifar-10-test.csv\n"
     ]
    }
   ],
   "source": [
    "# Remove the old training and test set from HDFS.\n",
    "!hdfs dfs -rm data/cifar-10-training.csv\n",
    "!hdfs dfs -rm data/cifar-10-test.csv\n",
    "# Copy the training and test set to HDFS.\n",
    "!hdfs dfs -copyFromLocal cifar-10-training.csv data/cifar-10-training.csv\n",
    "!hdfs dfs -copyFromLocal cifar-10-test.csv data/cifar-10-test.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further distributed preprocessing with Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up a Spark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Modify these variables according to your needs.\n",
    "application_name = \"CIFAR-10 Preprocessing Notebook\"\n",
    "using_spark_2 = False\n",
    "local = False\n",
    "path_train = \"data/cifar-10-training.csv\"\n",
    "path_test = \"data/cifar-10-test.csv\"\n",
    "if local:\n",
    "    # Tell master to use local resources.\n",
    "    master = \"local[*]\"\n",
    "    num_processes = 3\n",
    "    num_executors = 1\n",
    "else:\n",
    "    # Tell master to use YARN.\n",
    "    master = \"yarn-client\"\n",
    "    num_executors = 20\n",
    "    num_processes = 1\n",
    "    \n",
    "num_workers = num_executors * num_processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Use the DataBricks CSV reader, this has some nice functionality regarding invalid values.\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-csv_2.10:1.4.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conf = SparkConf()\n",
    "conf.set(\"spark.app.name\", application_name)\n",
    "conf.set(\"spark.master\", master)\n",
    "conf.set(\"spark.executor.cores\", `num_processes`)\n",
    "conf.set(\"spark.executor.instances\", `num_executors`)\n",
    "conf.set(\"spark.executor.memory\", \"4g\")\n",
    "conf.set(\"spark.locality.wait\", \"0\")\n",
    "conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\");\n",
    "\n",
    "# Check if the user is running Spark 2.0 +\n",
    "if using_spark_2:\n",
    "    sc = SparkSession.builder.config(conf=conf) \\\n",
    "            .appName(application_name) \\\n",
    "            .getOrCreate()\n",
    "else:\n",
    "    # Create the Spark context.\n",
    "    sc = SparkContext(conf=conf)\n",
    "    # Add the missing imports\n",
    "    from pyspark import SQLContext\n",
    "    sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the raw CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check if we are using Spark 2.0\n",
    "if using_spark_2:\n",
    "    reader = sc\n",
    "else:\n",
    "    reader = sqlContext\n",
    "# Read the training set.\n",
    "raw_dataset_train = reader.read.format('com.databricks.spark.csv') \\\n",
    "                          .options(header='true', inferSchema='true') \\\n",
    "                          .load(path_train)\n",
    "# Read the testing set.\n",
    "raw_dataset_test = reader.read.format('com.databricks.spark.csv') \\\n",
    "                         .options(header='true', inferSchema='true') \\\n",
    "                         .load(path_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 50000\n",
      "Test set size: 10000\n"
     ]
    }
   ],
   "source": [
    "# Count the number of instances in the training and test set (to check).\n",
    "print(\"Training set size: \" + str(raw_dataset_train.count()))\n",
    "print(\"Test set size: \" + str(raw_dataset_test.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing for further preprocessing, training and testing\n",
    "\n",
    "In order to ensure compatibility with Apache Spark, we vectorize the columns, and add the resulting vectors as a seperate column. However, in order to achieve this, we first need a list of the required columns. This is shown in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = raw_dataset_train.columns\n",
    "features.remove('label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have a list of columns names, we can pass this to Spark's [VectorAssembler](http://spark.apache.org/docs/latest/ml-features.html#vectorassembler). This VectorAssembler will take a list of features, vectorize them, and place them in a column defined in `outputCol`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Assemble the columns.\n",
    "vector_assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
    "dataset_train = vector_assembler.transform(raw_dataset_train)\n",
    "dataset_test = vector_assembler.transform(raw_dataset_test)\n",
    "# Repartition the dataset.\n",
    "dataset_train = dataset_train.repartition(num_workers)\n",
    "dataset_test = dataset_test.repartition(num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the inputs for our Neural Network (features column) after applying the VectorAssembler, we should also define the outputs. Since we are dealing with a classification task, the output of our Neural Network should be a one-hot encoded vector with 10 elements. For this, we provide a `OneHotTransformer` which accomplish this exact task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nb_classes = 10\n",
    "encoder = OneHotTransformer(nb_classes, input_col=\"label\", output_col=\"label_encoded\")\n",
    "dataset_train = encoder.transform(dataset_train)\n",
    "dataset_test = encoder.transform(dataset_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, normalize the pixel intensities with the range [0, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Allocate a MinMaxTransformer.\n",
    "transformer = MinMaxTransformer(n_min=0.0, n_max=1.0, \\\n",
    "                                o_min=0.0, o_max=250.0, \\\n",
    "                                input_col=\"features\", \\\n",
    "                                output_col=\"features_normalized\")\n",
    "# Transform the datasets.\n",
    "dataset_train = transformer.transform(dataset_train)\n",
    "dataset_test = transformer.transform(dataset_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the datasets to Parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted data/cifar-10-train-preprocessed.parquet\n",
      "Deleted data/cifar-10-test-preprocessed.parquet\n"
     ]
    }
   ],
   "source": [
    "# Delete the old preprocessed Parquet files.\n",
    "!hdfs dfs -rm -r data/cifar-10-train-preprocessed.parquet\n",
    "!hdfs dfs -rm -r data/cifar-10-test-preprocessed.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset_train.write.parquet(\"data/cifar-10-train-preprocessed.parquet\")\n",
    "dataset_test.write.parquet(\"data/cifar-10-test-preprocessed.parquet\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
